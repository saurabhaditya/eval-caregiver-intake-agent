<integration_task>
  <goal>
    Integrate this codebase with the caregiver intake agent evaluation framework
    by building an adapter that connects our running agent to the eval harness.
  </goal>

  <eval_framework>
    <repository>https://github.com/saurabhaditya/eval-caregiver-intake-agent</repository>
    <guide>https://github.com/saurabhaditya/eval-caregiver-intake-agent/blob/main/INTEGRATION.md</guide>
    <install>Add eval-caregiver as a dependency (git repo or local path)</install>
  </eval_framework>

  <!-- ================================================================
       BOUNDARIES — read this section BEFORE writing any code.
       These rules ensure the eval measures real agent quality,
       not how well the agent games the eval.
       ================================================================ -->
  <boundaries>
    <principle>
      The eval framework is a black-box tester. The agent is the system under test.
      The adapter is the ONLY component that touches both. The agent must have no
      awareness that it is being evaluated.
    </principle>

    <dependency_rule>
      eval_framework ◄── adapter ──► agent

      - The eval framework NEVER imports from the agent codebase.
      - The agent NEVER imports from the eval framework.
      - The adapter imports from both and translates between them.
    </dependency_rule>

    <forbidden_crossovers>
      <rule id="B1" severity="critical" name="no_expected_answers_to_agent">
        <what>expected_compliance_gaps, expected_geo_concerns, grader_names, required</what>
        <from>Scenario JSON / TestScenario</from>
        <must_not_reach>Agent's prompt, context, input, or any runtime state</must_not_reach>
        <why>
          If the agent sees expected answers, it can echo them back without reasoning.
          The eval becomes a tautology.
        </why>
        <enforcement>
          The adapter's _build_agent_input() must pass ONLY scenario.caregiver_setup
          (and optionally scenario.description) to the agent. It must NEVER pass
          expected_compliance_gaps, expected_geo_concerns, grader_names, or required.
        </enforcement>
      </rule>

      <rule id="B2" severity="critical" name="no_grader_logic_in_agent">
        <what>Grader scoring criteria, rubrics, pass/fail thresholds, quality gate thresholds</what>
        <from>graders/code_based/*.py, graders/model_based/*.py, quality_gates.py</from>
        <must_not_reach>Agent's system prompt, decision logic, or configuration</must_not_reach>
        <why>
          If the agent's prompt says "always set scheduling_offered=True" or
          "list remediation steps," it's teaching to the test, not demonstrating
          real capability.
        </why>
        <enforcement>
          The agent's prompts and instructions must be written to serve caregivers,
          not to satisfy grader field checks. Never reference grader names, scoring
          weights, or threshold values in agent code.
        </enforcement>
      </rule>

      <rule id="B3" severity="high" name="eval_schema_is_not_agent_data_model">
        <what>eval_caregiver.schemas.* (StructuredIntakeRecord, AgentActionLog, etc.)</what>
        <from>Eval framework</from>
        <must_not_reach>Agent's internal data model or runtime imports</must_not_reach>
        <why>
          If the agent constructs eval schema objects directly, it couples the agent
          to the eval. Changing the eval schema breaks the agent. The agent developer
          starts thinking in eval terms instead of caregiver terms.
        </why>
        <enforcement>
          The agent must use its own native data models. The adapter translates:
            agent.PatientRecord  →  _parse_intake()    →  StructuredIntakeRecord
            agent.ChatHistory    →  _parse_transcript() →  ConversationTranscript
            agent.StepLog        →  _parse_actions()    →  AgentActionLog
        </enforcement>
      </rule>

      <rule id="B4" severity="high" name="no_mock_responses_in_agent">
        <what>Mock agent responses (data/responses/*.json), mock conversation patterns</what>
        <from>Eval framework data/ directory</from>
        <must_not_reach>Agent training data, few-shot examples, or prompt engineering</must_not_reach>
        <why>
          Mock responses are ideal outputs that define what a perfect agent would do.
          Using them as few-shot examples makes the agent mimic eval expectations
          rather than developing genuine capability.
        </why>
        <enforcement>
          Never read data/responses/ from the agent codebase. Never copy mock
          conversation patterns into agent prompts.
        </enforcement>
      </rule>

      <rule id="B5" severity="high" name="no_agent_internals_in_graders">
        <what>Agent's prompt templates, tool call format, retry logic, LLM provider/model</what>
        <from>Agent codebase</from>
        <must_not_reach>Grader logic or eval framework assumptions</must_not_reach>
        <why>
          Graders must evaluate WHAT the agent produced, not HOW it produced it.
          If a grader checks "did the agent use the check_compliance tool," a
          different agent architecture that achieves the same result would fail.
        </why>
        <enforcement>
          Graders read only the three AgentOutput fields (transcript, intake_record,
          action_log). Never add grader logic that depends on agent implementation details.
        </enforcement>
      </rule>
    </forbidden_crossovers>

    <litmus_tests>
      <test>Could the agent pass this eval without actually being good at intake? → You've leaked grader logic into the agent.</test>
      <test>Would changing the eval schema require changing the agent? → The agent is coupled to eval internals.</test>
      <test>Does the agent behave differently when it knows it's being evaluated? → The eval isn't measuring real behavior.</test>
      <test>Does a grader assume a specific agent architecture? → The grader is testing implementation, not outcomes.</test>
      <test>Are mock responses used anywhere in agent training? → You're teaching to the test.</test>
    </litmus_tests>
  </boundaries>

  <!-- ================================================================
       ADAPTER — the bridge between the two systems.
       ================================================================ -->
  <adapter>
    <class_name>LiveIntakeAgent</class_name>
    <base_class>eval_caregiver.agent.base.AgentBase</base_class>
    <method name="run_scenario">
      <signature>run_scenario(scenario: TestScenario) -> AgentOutput</signature>
      <description>
        Receives a test scenario, drives our agent through the conversation,
        and returns structured evaluation output.
      </description>
    </method>

    <pipeline>
      <step order="1" name="read_scenario">
        <description>Read scenario.caregiver_setup to understand the simulated caregiver state.</description>
        <input>
          <field name="scenario.caregiver_setup" type="dict">
            Scenario-specific config. Examples:
              {"cpr_status": "missing"}
              {"preferred_zones": ["zone-a"], "excluded_zones": ["zone-b", ...]}
              {"desired_hourly_rate": 32, "minimum_weekly_hours": 35}
          </field>
        </input>
        <boundary_check ref="B1">
          ONLY pass caregiver_setup (and optionally description) to the agent.
          STRIP: expected_compliance_gaps, expected_geo_concerns, grader_names, required.
        </boundary_check>
      </step>

      <step order="2" name="invoke_agent">
        <description>
          Call our existing agent with the scenario setup to produce a real conversation.
          Map scenario.caregiver_setup into whatever input format our agent expects.
        </description>
        <boundary_check ref="B3">
          The agent must use its own native input format, not eval schemas.
          The adapter translates caregiver_setup into the agent's expected format.
        </boundary_check>
      </step>

      <step order="3" name="map_outputs">
        <description>
          Parse the agent's NATIVE output into the three required eval objects.
          This is a pure translation layer — never modify the agent to emit eval schemas directly.
        </description>
        <boundary_check ref="B3">
          Translate from agent-native models to eval schemas. The agent must not
          import or construct eval_caregiver.schemas objects itself.
        </boundary_check>

        <output name="ConversationTranscript">
          <module>eval_caregiver.schemas.conversation</module>
          <mapping>
            Map our agent's native message format to ConversationTurn objects.
            Each turn needs: role ("agent" | "caregiver"), content (string), turn_number (1-indexed int).
          </mapping>
          <fields>
            <field name="scenario_id" type="str">From scenario.scenario_id</field>
            <field name="turns" type="list[ConversationTurn]">Ordered conversation messages</field>
          </fields>
        </output>

        <output name="StructuredIntakeRecord">
          <module>eval_caregiver.schemas.caregiver</module>
          <mapping>
            Extract structured data from our agent's output.
            Map our agent's native structured output (profile, gaps, actions) into the eval schema fields.
          </mapping>
          <fields>
            <field name="caregiver" type="CaregiverProfile">
              <subfields>
                <field name="caregiver_id" type="str" />
                <field name="full_name" type="str" />
                <field name="compliance" type="list[ComplianceRecord]">
                  Each record: item_name (str), status ("valid"|"expired"|"missing"|"unknown"),
                  expiration_date (date|null), notes (str)
                </field>
                <field name="geo_preferences" type="GeoPreferences">
                  preferred_zones, excluded_zones, max_travel_minutes, has_own_transport
                </field>
              </subfields>
            </field>
            <field name="compliance_gaps" type="list[str]">Names of missing/expired items, e.g. ["CPR Certification"]</field>
            <field name="remediation_actions" type="list[str]">Actions the agent offered, e.g. ["Schedule CPR class"]</field>
            <field name="geo_concerns" type="list[str]">e.g. ["over_restricted_zones", "limited_assignment_availability"]</field>
            <field name="safe_area_suggestions" type="list[str]">e.g. ["Zone B (low risk)", "Zone C (low risk)"]</field>
            <field name="overall_status" type="str">"complete" | "incomplete" | "needs_review"</field>
          </fields>
        </output>

        <output name="AgentActionLog">
          <module>eval_caregiver.schemas.conversation</module>
          <mapping>
            Build from our agent's internal action history or infer from the conversation.
            If the transcript mentions "I'll schedule a class", set scheduling_offered=True.
          </mapping>
          <fields>
            <field name="scenario_id" type="str">From scenario.scenario_id</field>
            <field name="actions" type="list[str]">Free-text action descriptions</field>
            <field name="compliance_items_checked" type="list[str]">Item names the agent reviewed</field>
            <field name="safety_map_consulted" type="bool">Did the agent look at safety data?</field>
            <field name="scheduling_offered" type="bool">Did the agent offer to schedule something?</field>
            <field name="remediation_steps_offered" type="list[str]">Remediation actions proposed</field>
          </fields>
        </output>
      </step>
    </pipeline>
  </adapter>

  <!-- ================================================================
       GRADER CONTRACTS — what each grader reads from AgentOutput.
       This section is for the ADAPTER DEVELOPER only.
       Never expose this information to the agent.
       ================================================================ -->
  <grader_contracts>
    <description>
      These are the specific fields each grader inspects.
      The adapter's output mapping MUST populate these correctly for grading to work.
      BOUNDARY: This information is for the adapter developer ONLY.
      Never incorporate grader field names, thresholds, or scoring logic into the agent.
    </description>

    <grader name="compliance_gap_detection" type="code_based">
      <reads>intake_record.compliance_gaps</reads>
      <compares_against>scenario.expected_compliance_gaps</compares_against>
      <logic>Recall: detected / expected. Passes if recall >= 0.95</logic>
    </grader>

    <grader name="compliance_remediation" type="code_based">
      <reads>intake_record.remediation_actions</reads>
      <reads>action_log.scheduling_offered</reads>
      <reads>action_log.remediation_steps_offered</reads>
      <logic>Average of 3 signals (has actions, scheduling offered, has steps). Passes if >= 0.85</logic>
    </grader>

    <grader name="geo_restriction_detection" type="code_based">
      <reads>intake_record.geo_concerns</reads>
      <reads>intake_record.safe_area_suggestions</reads>
      <reads>action_log.safety_map_consulted</reads>
      <compares_against>scenario.expected_geo_concerns</compares_against>
      <logic>Weighted: concern recall (50%) + has suggestions (25%) + map consulted (25%). Passes if >= 0.80</logic>
    </grader>

    <grader name="scheduling_helpfulness" type="model_based" requires="ANTHROPIC_API_KEY">
      <reads>transcript.full_text</reads>
      <rubric>Clarity, options offered, empathy (each 0-2 scale)</rubric>
    </grader>

    <grader name="safe_area_suggestion_quality" type="model_based" requires="ANTHROPIC_API_KEY">
      <reads>transcript.full_text</reads>
      <rubric>Map referenced, nearby safe areas identified, actionable next step (each 0-2 scale)</rubric>
    </grader>
  </grader_contracts>

  <runner>
    <description>Add a script or CLI command to execute the eval against our live agent.</description>
    <example><![CDATA[
from eval_caregiver.runner.executor import EvalExecutor
from eval_caregiver.scenarios.loader import get_all_scenarios
from our_adapter import LiveIntakeAgent

agent = LiveIntakeAgent()
# Build grader registry same as eval framework's cli.py
executor = EvalExecutor(agent=agent, graders=graders, skip_model_graders=True)
results = executor.run_scenarios(get_all_scenarios())
    ]]></example>
  </runner>

  <instructions>
    <instruction priority="1">
      Read our agent's existing code first to understand its input/output format.
    </instruction>
    <instruction priority="2">
      Build the adapter that bridges our agent's native interface to the eval schema.
      The adapter is the ONLY file that imports from both systems.
    </instruction>
    <instruction priority="3">
      In _build_agent_input(), pass ONLY scenario.caregiver_setup to the agent.
      NEVER pass expected_compliance_gaps, expected_geo_concerns, grader_names,
      or required — these are eval-only fields. (Boundary rule B1)
    </instruction>
    <instruction priority="4">
      Do NOT modify the agent's prompts, system instructions, or tool definitions
      to reference eval grader fields like "scheduling_offered" or "compliance_gaps."
      The agent must produce good intake conversations for caregivers, not for graders. (Boundary rule B2)
    </instruction>
    <instruction priority="5">
      Do NOT have the agent import eval_caregiver.schemas. The agent uses its own
      data models. The adapter translates between them. (Boundary rule B3)
    </instruction>
    <instruction priority="6">
      Do NOT use data/responses/*.json as few-shot examples or training data
      for the agent. (Boundary rule B4)
    </instruction>
    <instruction priority="7">
      If our agent lacks an explicit action log, infer the AgentActionLog fields
      from the conversation content in the adapter — not by adding logging to the
      agent that's shaped by what graders expect. (Boundary rule B5)
    </instruction>
    <instruction priority="8">
      Start with one collection for testing:
      uv run eval-runner --live -c compliance_missing_cases --scorecard --no-model-graders
    </instruction>
  </instructions>
</integration_task>
